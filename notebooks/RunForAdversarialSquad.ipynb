{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RunForAdversarialSquad.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdqDmcV2KeuB"
      },
      "source": [
        "This notebook runs the adversarial examples for SQuAD. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh4_940yftxv"
      },
      "source": [
        "Running BERT, AlBERT and Roberta\n",
        "Resources: \n",
        "  1. [Getting started QA notebook](https://colab.research.google.com/github/fastforwardlabs/ff14_blog/blob/master/_notebooks/2020-05-19-Getting_Started_with_QA.ipynb#scrollTo=p9HJO7j1gnxc)\n",
        "  2. [Bert SQuad](https://colab.research.google.com/drive/1Xph-1GLUf4BRzCD9UXWY7EphRi2t9cBj#scrollTo=3SyG8rzO-K-w)\n",
        "  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xa10YE-1LBAn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b63ddda1-34b5-4a72-9e55-2ca014fc8aac"
      },
      "source": [
        "!pip install torch  torchvision -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.7.0+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.8.1+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch) (0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/40/866cbfac4601e0f74c7303d533a9c5d4a53858bd402e08e3e294dd271f25/transformers-4.2.1-py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 4.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 25.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 28.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=e10482de3113ac5df36e054742ded2fbe49b26a76d50ae3d77934c55b2397645\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.2.1\n",
            "Collecting datasets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/9b/d097f2238fc3c028495cf5f8c65378972b9f1b2cbb27f3c57c7219195aa9/datasets-1.2.1-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 5.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from datasets) (0.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from datasets) (3.3.0)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.6/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets) (0.3.3)\n",
            "Collecting pyarrow>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e1/27958a70848f8f7089bff8d6ebe42519daf01f976d28b481e1bfd52c8097/pyarrow-2.0.0-cp36-cp36m-manylinux2014_x86_64.whl (17.7MB)\n",
            "\u001b[K     |████████████████████████████████| 17.7MB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.23.0)\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/73/826b19f3594756cb1c6c23d2fbd8ca6a77a9cd3b650c9dec5acc85004c38/xxhash-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (242kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 36.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: pyarrow, xxhash, datasets\n",
            "  Found existing installation: pyarrow 0.14.1\n",
            "    Uninstalling pyarrow-0.14.1:\n",
            "      Successfully uninstalled pyarrow-0.14.1\n",
            "Successfully installed datasets-1.2.1 pyarrow-2.0.0 xxhash-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rGB3BGYtpsFT",
        "outputId": "2e4b30d3-777f-4de3-b210-a3c4694cbde6"
      },
      "source": [
        "pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UNzxfJUquPH",
        "outputId": "89ec366b-9ada-47cf-eb19-059902741046"
      },
      "source": [
        "cd drive/MyDrive/cmpe588-project/bert-runs/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/cmpe588-project/bert-runs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2ypGqbwK2Ic",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76b48dc2-2978-4116-e77c-fc7853622a9b"
      },
      "source": [
        "# set path with magic\n",
        "%env DATA_DIR=./data/squad \n",
        "\n",
        "# download the data\n",
        "def download_squad(version=1):\n",
        "    if version == 1:\n",
        "        !wget -P $DATA_DIR https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\n",
        "        !wget -P $DATA_DIR https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\n",
        "    else:\n",
        "        !wget -P $DATA_DIR https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
        "        !wget -P $DATA_DIR https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\n",
        "            \n",
        "download_squad(version=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: DATA_DIR=./data/squad\n",
            "--2021-01-18 13:24:35--  https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 30288272 (29M) [application/json]\n",
            "Saving to: ‘./data/squad/train-v1.1.json.1’\n",
            "\n",
            "train-v1.1.json.1   100%[===================>]  28.88M  52.1MB/s    in 0.6s    \n",
            "\n",
            "2021-01-18 13:24:36 (52.1 MB/s) - ‘./data/squad/train-v1.1.json.1’ saved [30288272/30288272]\n",
            "\n",
            "--2021-01-18 13:24:36--  https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4854279 (4.6M) [application/json]\n",
            "Saving to: ‘./data/squad/dev-v1.1.json.1’\n",
            "\n",
            "dev-v1.1.json.1     100%[===================>]   4.63M  28.8MB/s    in 0.2s    \n",
            "\n",
            "2021-01-18 13:24:36 (28.8 MB/s) - ‘./data/squad/dev-v1.1.json.1’ saved [4854279/4854279]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ylu-eeqIpAAC"
      },
      "source": [
        "**BERT MODEL on Adversarial SQUAD**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtWZMPYXpezk"
      },
      "source": [
        "First, running the finetuned BERT model on the sampled test set from [AdversarialSQuAD](https://worksheets.codalab.org/worksheets/0xc86d3ebe69a3427d91f9aaa63f7d1e7d/)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajJrZvrLpOJm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6626362a-362e-46e7-c410-8287246b5975"
      },
      "source": [
        "!python run_qa.py \\\n",
        "  --model_name_or_path bert-large-uncased-whole-word-masking-finetuned-squad \\\n",
        "  --dataset_name squad \\\n",
        "  --train_file ./data/squad/train-v1.1.json \\\n",
        "  --validation_file ./data/squad/sample1k-HCVerifyAll.json \\\n",
        "  --do_eval \\\n",
        "  --per_device_train_batch_size 12 \\\n",
        "  --learning_rate 3e-5 \\\n",
        "  --max_seq_length 384 \\\n",
        "  --doc_stride 128 \\\n",
        "  --output_dir ./tmp/debug_squad/ \\\n",
        "  --overwrite_output_dir"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-01-20 05:12:28.482506: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "01/20/2021 05:12:33 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
            "01/20/2021 05:12:33 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=./tmp/debug_squad/, overwrite_output_dir=True, do_train=False, do_eval=True, do_predict=False, evaluation_strategy=EvaluationStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=12, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=3e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_steps=0, logging_dir=runs/Jan20_05-12-32_296ae2a9f8f2, logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=./tmp/debug_squad/, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, _n_gpu=0)\n",
            "Using custom data configuration default\n",
            "Downloading and preparing dataset squad/default-549ec43428507c2c (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/squad/default-549ec43428507c2c/0.0.0/f5babbce8c0f200bb625d7847c74110b6da7f1c2815b940aff2c6da4459f5a83...\n",
            "Dataset squad downloaded and prepared to /root/.cache/huggingface/datasets/squad/default-549ec43428507c2c/0.0.0/f5babbce8c0f200bb625d7847c74110b6da7f1c2815b940aff2c6da4459f5a83. Subsequent calls will reuse this data.\n",
            "https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpcorsm6_v\n",
            "Downloading: 100% 443/443 [00:00<00:00, 337kB/s]\n",
            "storing https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/402f6d8c99fdd3bffd354782842e2b5a6be81f80ab630591051ebc78ca726f39.ebffac96fee44dbe30674c204dd3d3f358c1b8c33100281ecdd688514f41410a\n",
            "creating metadata file for /root/.cache/huggingface/transformers/402f6d8c99fdd3bffd354782842e2b5a6be81f80ab630591051ebc78ca726f39.ebffac96fee44dbe30674c204dd3d3f358c1b8c33100281ecdd688514f41410a\n",
            "loading configuration file https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/402f6d8c99fdd3bffd354782842e2b5a6be81f80ab630591051ebc78ca726f39.ebffac96fee44dbe30674c204dd3d3f358c1b8c33100281ecdd688514f41410a\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForQuestionAnswering\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.2.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/402f6d8c99fdd3bffd354782842e2b5a6be81f80ab630591051ebc78ca726f39.ebffac96fee44dbe30674c204dd3d3f358c1b8c33100281ecdd688514f41410a\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForQuestionAnswering\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.2.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmps4xo7k8s\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 2.09MB/s]\n",
            "storing https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/68e5260dea718cdc2daf27dc106fd8741636b03e3173b5492e57a7fa525ca33b.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "creating metadata file for /root/.cache/huggingface/transformers/68e5260dea718cdc2daf27dc106fd8741636b03e3173b5492e57a7fa525ca33b.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpcuj7o_aw\n",
            "Downloading: 100% 466k/466k [00:00<00:00, 3.32MB/s]\n",
            "storing https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/9b7535fe1c0da28aa7cc66b7f34529d984f535c401be8352f6adeb25f7870def.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "creating metadata file for /root/.cache/huggingface/transformers/9b7535fe1c0da28aa7cc66b7f34529d984f535c401be8352f6adeb25f7870def.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading file https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/68e5260dea718cdc2daf27dc106fd8741636b03e3173b5492e57a7fa525ca33b.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/9b7535fe1c0da28aa7cc66b7f34529d984f535c401be8352f6adeb25f7870def.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpoqhu1xo_\n",
            "Downloading: 100% 1.34G/1.34G [00:17<00:00, 78.5MB/s]\n",
            "storing https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/28a060c1e2e1216bd9c8f5222ce38ce916c4829b8b05e027fe91510f3fd4da7e.50fc4a146342b3a6a99b185af3d5b70163b64d45790be64d9124dcccbcd3915e\n",
            "creating metadata file for /root/.cache/huggingface/transformers/28a060c1e2e1216bd9c8f5222ce38ce916c4829b8b05e027fe91510f3fd4da7e.50fc4a146342b3a6a99b185af3d5b70163b64d45790be64d9124dcccbcd3915e\n",
            "loading weights file https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/28a060c1e2e1216bd9c8f5222ce38ce916c4829b8b05e027fe91510f3fd4da7e.50fc4a146342b3a6a99b185af3d5b70163b64d45790be64d9124dcccbcd3915e\n",
            "All model checkpoint weights were used when initializing BertForQuestionAnswering.\n",
            "\n",
            "All the weights of BertForQuestionAnswering were initialized from the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n",
            "100% 4/4 [00:07<00:00,  1.97s/ba]\n",
            "Downloading: 4.02kB [00:00, 2.75MB/s]       \n",
            "Downloading: 3.35kB [00:00, 2.18MB/s]       \n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
            "01/20/2021 05:13:25 - INFO - __main__ -   *** Evaluate ***\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3673\n",
            "  Batch size = 8\n",
            "100% 460/460 [4:23:46<00:00, 25.68s/it]01/20/2021 09:37:49 - INFO - utils_qa -   Post-processing 3560 example predictions split into 3673 features.\n",
            "\n",
            "  0% 0/3560 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 42/3560 [00:00<00:08, 413.94it/s]\u001b[A\n",
            "  3% 90/3560 [00:00<00:08, 431.40it/s]\u001b[A\n",
            "  3% 120/3560 [00:00<00:09, 379.36it/s]\u001b[A\n",
            "  4% 160/3560 [00:00<00:08, 383.91it/s]\u001b[A\n",
            "  6% 207/3560 [00:00<00:08, 400.38it/s]\u001b[A\n",
            "  7% 253/3560 [00:00<00:07, 415.33it/s]\u001b[A\n",
            "  8% 299/3560 [00:00<00:07, 425.05it/s]\u001b[A\n",
            " 10% 339/3560 [00:00<00:07, 403.05it/s]\u001b[A\n",
            " 11% 378/3560 [00:00<00:08, 389.33it/s]\u001b[A\n",
            " 12% 418/3560 [00:01<00:08, 390.31it/s]\u001b[A\n",
            " 13% 457/3560 [00:01<00:08, 383.42it/s]\u001b[A\n",
            " 14% 501/3560 [00:01<00:07, 396.04it/s]\u001b[A\n",
            " 15% 543/3560 [00:01<00:07, 400.21it/s]\u001b[A\n",
            " 17% 589/3560 [00:01<00:07, 415.75it/s]\u001b[A\n",
            " 18% 632/3560 [00:01<00:06, 419.41it/s]\u001b[A\n",
            " 19% 680/3560 [00:01<00:06, 433.46it/s]\u001b[A\n",
            " 20% 724/3560 [00:01<00:06, 426.02it/s]\u001b[A\n",
            " 22% 767/3560 [00:01<00:06, 427.20it/s]\u001b[A\n",
            " 23% 810/3560 [00:01<00:06, 413.80it/s]\u001b[A\n",
            " 24% 852/3560 [00:02<00:07, 378.23it/s]\u001b[A\n",
            " 25% 896/3560 [00:02<00:06, 392.59it/s]\u001b[A\n",
            " 26% 937/3560 [00:02<00:06, 396.28it/s]\u001b[A\n",
            " 27% 978/3560 [00:02<00:06, 390.11it/s]\u001b[A\n",
            " 29% 1018/3560 [00:02<00:06, 382.98it/s]\u001b[A\n",
            " 30% 1057/3560 [00:02<00:06, 382.91it/s]\u001b[A\n",
            " 31% 1100/3560 [00:02<00:06, 393.86it/s]\u001b[A\n",
            " 32% 1141/3560 [00:02<00:06, 397.90it/s]\u001b[A\n",
            " 33% 1181/3560 [00:02<00:06, 384.23it/s]\u001b[A\n",
            " 34% 1220/3560 [00:03<00:06, 384.36it/s]\u001b[A\n",
            " 35% 1259/3560 [00:03<00:06, 381.82it/s]\u001b[A\n",
            " 37% 1300/3560 [00:03<00:05, 388.73it/s]\u001b[A\n",
            " 38% 1341/3560 [00:03<00:05, 394.60it/s]\u001b[A\n",
            " 39% 1381/3560 [00:03<00:05, 375.65it/s]\u001b[A\n",
            " 40% 1419/3560 [00:03<00:06, 310.11it/s]\u001b[A\n",
            " 41% 1453/3560 [00:03<00:08, 243.12it/s]\u001b[A\n",
            " 42% 1483/3560 [00:03<00:08, 256.50it/s]\u001b[A\n",
            " 43% 1523/3560 [00:04<00:07, 286.25it/s]\u001b[A\n",
            " 44% 1565/3560 [00:04<00:06, 314.44it/s]\u001b[A\n",
            " 45% 1604/3560 [00:04<00:05, 332.12it/s]\u001b[A\n",
            " 46% 1646/3560 [00:04<00:05, 354.34it/s]\u001b[A\n",
            " 47% 1687/3560 [00:04<00:05, 368.22it/s]\u001b[A\n",
            " 48% 1726/3560 [00:04<00:04, 371.32it/s]\u001b[A\n",
            " 50% 1765/3560 [00:04<00:04, 374.89it/s]\u001b[A\n",
            " 51% 1806/3560 [00:04<00:04, 382.95it/s]\u001b[A\n",
            " 52% 1846/3560 [00:04<00:04, 385.92it/s]\u001b[A\n",
            " 53% 1886/3560 [00:04<00:04, 380.55it/s]\u001b[A\n",
            " 54% 1925/3560 [00:05<00:04, 376.63it/s]\u001b[A\n",
            " 55% 1964/3560 [00:05<00:04, 380.37it/s]\u001b[A\n",
            " 56% 2003/3560 [00:05<00:04, 375.03it/s]\u001b[A\n",
            " 57% 2045/3560 [00:05<00:03, 386.08it/s]\u001b[A\n",
            " 59% 2084/3560 [00:05<00:03, 386.69it/s]\u001b[A\n",
            " 60% 2123/3560 [00:05<00:03, 379.08it/s]\u001b[A\n",
            " 61% 2162/3560 [00:05<00:03, 375.20it/s]\u001b[A\n",
            " 62% 2200/3560 [00:05<00:03, 370.72it/s]\u001b[A\n",
            " 63% 2241/3560 [00:05<00:03, 380.84it/s]\u001b[A\n",
            " 64% 2280/3560 [00:06<00:03, 372.46it/s]\u001b[A\n",
            " 65% 2318/3560 [00:06<00:03, 368.66it/s]\u001b[A\n",
            " 66% 2357/3560 [00:06<00:03, 374.18it/s]\u001b[A\n",
            " 67% 2399/3560 [00:06<00:03, 385.83it/s]\u001b[A\n",
            " 69% 2443/3560 [00:06<00:02, 399.54it/s]\u001b[A\n",
            " 70% 2484/3560 [00:06<00:02, 377.48it/s]\u001b[A\n",
            " 71% 2523/3560 [00:06<00:02, 374.29it/s]\u001b[A\n",
            " 72% 2564/3560 [00:06<00:02, 383.59it/s]\u001b[A\n",
            " 73% 2603/3560 [00:06<00:02, 384.20it/s]\u001b[A\n",
            " 74% 2644/3560 [00:06<00:02, 391.13it/s]\u001b[A\n",
            " 75% 2684/3560 [00:07<00:02, 339.15it/s]\u001b[A\n",
            " 76% 2722/3560 [00:07<00:02, 349.13it/s]\u001b[A\n",
            " 78% 2761/3560 [00:07<00:02, 357.90it/s]\u001b[A\n",
            " 79% 2800/3560 [00:07<00:02, 364.92it/s]\u001b[A\n",
            " 80% 2838/3560 [00:07<00:01, 367.00it/s]\u001b[A\n",
            " 81% 2877/3560 [00:07<00:01, 371.98it/s]\u001b[A\n",
            " 82% 2918/3560 [00:07<00:01, 380.04it/s]\u001b[A\n",
            " 83% 2959/3560 [00:07<00:01, 387.60it/s]\u001b[A\n",
            " 84% 2998/3560 [00:07<00:01, 382.89it/s]\u001b[A\n",
            " 85% 3039/3560 [00:08<00:01, 389.86it/s]\u001b[A\n",
            " 86% 3079/3560 [00:08<00:01, 369.23it/s]\u001b[A\n",
            " 88% 3117/3560 [00:08<00:01, 371.65it/s]\u001b[A\n",
            " 89% 3160/3560 [00:08<00:01, 386.24it/s]\u001b[A\n",
            " 90% 3202/3560 [00:08<00:00, 393.42it/s]\u001b[A\n",
            " 91% 3243/3560 [00:08<00:00, 396.32it/s]\u001b[A\n",
            " 92% 3287/3560 [00:08<00:00, 406.64it/s]\u001b[A\n",
            " 94% 3331/3560 [00:08<00:00, 414.05it/s]\u001b[A\n",
            " 95% 3373/3560 [00:08<00:00, 394.45it/s]\u001b[A\n",
            " 96% 3416/3560 [00:08<00:00, 403.22it/s]\u001b[A\n",
            " 97% 3457/3560 [00:09<00:00, 399.78it/s]\u001b[A\n",
            " 98% 3498/3560 [00:09<00:00, 391.19it/s]\u001b[A\n",
            "100% 3560/3560 [00:09<00:00, 380.83it/s]\n",
            "01/20/2021 09:37:59 - INFO - utils_qa -   Saving predictions to ./tmp/debug_squad/predictions.json.\n",
            "01/20/2021 09:37:59 - INFO - utils_qa -   Saving nbest_preds to ./tmp/debug_squad/nbest_predictions.json.\n",
            "100% 460/460 [4:24:01<00:00, 34.44s/it]\n",
            "01/20/2021 09:38:01 - INFO - __main__ -   ***** Eval results *****\n",
            "01/20/2021 09:38:01 - INFO - __main__ -     exact_match = 72.3314606741573\n",
            "01/20/2021 09:38:01 - INFO - __main__ -     f1 = 78.03553109451705\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfR3putkqVKe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "402bfbd4-25cd-48e1-e5c3-180048ec61c3"
      },
      "source": [
        "!python run_qa.py \\\n",
        "  --model_name_or_path bert-large-uncased-whole-word-masking-finetuned-squad \\\n",
        "  --dataset_name squad \\\n",
        "  --train_file ./data/squad/train-v1.1.json \\\n",
        "  --validation_file ./data/squad/sample1k-HCVerifySample.json \\\n",
        "  --do_eval \\\n",
        "  --per_device_train_batch_size 12 \\\n",
        "  --learning_rate 3e-5 \\\n",
        "  --max_seq_length 384 \\\n",
        "  --doc_stride 128 \\\n",
        "  --output_dir ./tmp/debug_squad_verifySample/ \\\n",
        "  --overwrite_output_dir"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-01-18 22:49:10.771527: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "01/18/2021 22:49:12 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
            "01/18/2021 22:49:12 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=./tmp/debug_squad/, overwrite_output_dir=True, do_train=False, do_eval=True, do_predict=False, evaluation_strategy=EvaluationStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=12, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=3e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_steps=0, logging_dir=runs/Jan18_22-49-12_2097f6b1fa54, logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=./tmp/debug_squad/, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, _n_gpu=0)\n",
            "Using custom data configuration default\n",
            "Downloading and preparing dataset squad/default-33171079d281940c (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/squad/default-33171079d281940c/0.0.0/f5babbce8c0f200bb625d7847c74110b6da7f1c2815b940aff2c6da4459f5a83...\n",
            "Dataset squad downloaded and prepared to /root/.cache/huggingface/datasets/squad/default-33171079d281940c/0.0.0/f5babbce8c0f200bb625d7847c74110b6da7f1c2815b940aff2c6da4459f5a83. Subsequent calls will reuse this data.\n",
            "loading configuration file https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/402f6d8c99fdd3bffd354782842e2b5a6be81f80ab630591051ebc78ca726f39.ebffac96fee44dbe30674c204dd3d3f358c1b8c33100281ecdd688514f41410a\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForQuestionAnswering\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.2.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/402f6d8c99fdd3bffd354782842e2b5a6be81f80ab630591051ebc78ca726f39.ebffac96fee44dbe30674c204dd3d3f358c1b8c33100281ecdd688514f41410a\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForQuestionAnswering\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.2.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/68e5260dea718cdc2daf27dc106fd8741636b03e3173b5492e57a7fa525ca33b.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/9b7535fe1c0da28aa7cc66b7f34529d984f535c401be8352f6adeb25f7870def.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading weights file https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/28a060c1e2e1216bd9c8f5222ce38ce916c4829b8b05e027fe91510f3fd4da7e.50fc4a146342b3a6a99b185af3d5b70163b64d45790be64d9124dcccbcd3915e\n",
            "All model checkpoint weights were used when initializing BertForQuestionAnswering.\n",
            "\n",
            "All the weights of BertForQuestionAnswering were initialized from the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n",
            "100% 2/2 [00:04<00:00,  2.19s/ba]\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
            "01/18/2021 22:49:40 - INFO - __main__ -   *** Evaluate ***\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1839\n",
            "  Batch size = 8\n",
            "100% 230/230 [2:12:25<00:00, 33.55s/it]01/19/2021 01:02:42 - INFO - utils_qa -   Post-processing 1787 example predictions split into 1839 features.\n",
            "\n",
            "  0% 0/1787 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 43/1787 [00:00<00:04, 419.70it/s]\u001b[A\n",
            "  4% 78/1787 [00:00<00:04, 395.61it/s]\u001b[A\n",
            "  7% 118/1787 [00:00<00:04, 395.03it/s]\u001b[A\n",
            "  9% 163/1787 [00:00<00:03, 407.00it/s]\u001b[A\n",
            " 11% 201/1787 [00:00<00:03, 398.38it/s]\u001b[A\n",
            " 13% 239/1787 [00:00<00:03, 392.48it/s]\u001b[A\n",
            " 16% 279/1787 [00:00<00:03, 393.03it/s]\u001b[A\n",
            " 18% 325/1787 [00:00<00:03, 408.75it/s]\u001b[A\n",
            " 21% 371/1787 [00:00<00:03, 420.05it/s]\u001b[A\n",
            " 23% 412/1787 [00:01<00:03, 415.67it/s]\u001b[A\n",
            " 25% 453/1787 [00:01<00:03, 408.04it/s]\u001b[A\n",
            " 28% 499/1787 [00:01<00:03, 420.44it/s]\u001b[A\n",
            " 30% 541/1787 [00:01<00:03, 403.80it/s]\u001b[A\n",
            " 33% 582/1787 [00:01<00:03, 391.79it/s]\u001b[A\n",
            " 35% 625/1787 [00:01<00:02, 400.25it/s]\u001b[A\n",
            " 37% 666/1787 [00:01<00:02, 397.79it/s]\u001b[A\n",
            " 40% 706/1787 [00:01<00:02, 364.59it/s]\u001b[A\n",
            " 42% 744/1787 [00:01<00:03, 311.07it/s]\u001b[A\n",
            " 44% 782/1787 [00:02<00:03, 328.44it/s]\u001b[A\n",
            " 46% 817/1787 [00:02<00:02, 334.16it/s]\u001b[A\n",
            " 48% 857/1787 [00:02<00:02, 351.17it/s]\u001b[A\n",
            " 50% 894/1787 [00:02<00:02, 350.13it/s]\u001b[A\n",
            " 52% 934/1787 [00:02<00:02, 363.20it/s]\u001b[A\n",
            " 55% 974/1787 [00:02<00:02, 371.77it/s]\u001b[A\n",
            " 57% 1016/1787 [00:02<00:02, 384.51it/s]\u001b[A\n",
            " 59% 1057/1787 [00:02<00:01, 391.05it/s]\u001b[A\n",
            " 61% 1097/1787 [00:02<00:01, 382.56it/s]\u001b[A\n",
            " 64% 1137/1787 [00:02<00:01, 387.60it/s]\u001b[A\n",
            " 66% 1179/1787 [00:03<00:01, 395.08it/s]\u001b[A\n",
            " 68% 1219/1787 [00:03<00:01, 393.80it/s]\u001b[A\n",
            " 70% 1259/1787 [00:03<00:01, 388.49it/s]\u001b[A\n",
            " 73% 1298/1787 [00:03<00:01, 386.59it/s]\u001b[A\n",
            " 75% 1337/1787 [00:03<00:01, 369.66it/s]\u001b[A\n",
            " 77% 1379/1787 [00:03<00:01, 381.40it/s]\u001b[A\n",
            " 79% 1418/1787 [00:03<00:00, 375.71it/s]\u001b[A\n",
            " 82% 1458/1787 [00:03<00:00, 382.61it/s]\u001b[A\n",
            " 84% 1500/1787 [00:03<00:00, 392.69it/s]\u001b[A\n",
            " 86% 1541/1787 [00:04<00:00, 394.83it/s]\u001b[A\n",
            " 88% 1581/1787 [00:04<00:00, 381.77it/s]\u001b[A\n",
            " 91% 1625/1787 [00:04<00:00, 395.42it/s]\u001b[A\n",
            " 93% 1670/1787 [00:04<00:00, 408.02it/s]\u001b[A\n",
            " 96% 1712/1787 [00:04<00:00, 394.67it/s]\u001b[A\n",
            "100% 1787/1787 [00:04<00:00, 386.94it/s]\n",
            "01/19/2021 01:02:46 - INFO - utils_qa -   Saving predictions to ./tmp/debug_squad/predictions.json.\n",
            "01/19/2021 01:02:46 - INFO - utils_qa -   Saving nbest_preds to ./tmp/debug_squad/nbest_predictions.json.\n",
            "100% 230/230 [2:12:32<00:00, 34.58s/it]\n",
            "01/19/2021 01:02:47 - INFO - __main__ -   ***** Eval results *****\n",
            "01/19/2021 01:02:47 - INFO - __main__ -     exact_match = 78.23167319529938\n",
            "01/19/2021 01:02:47 - INFO - __main__ -     f1 = 84.01109980628269\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DjPR1jkuqec"
      },
      "source": [
        "Eval_squad results for Distilled on HCVeridySample {\"orig_exact_match\": 86.7, \"orig_f1\": 92.83015182841169, \"adv_exact_match\": 71.7, \"adv_f1\": 76.92238678998721}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8bVUm96Cels"
      },
      "source": [
        "Running finetuned DistilBert from [distilbert-base-cased-distilled-squad](https://huggingface.co/distilbert-base-cased-distilled-squad)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRIJPsUBCceS",
        "outputId": "0bcd666c-9a4c-42d2-c8d1-4d1b7895f0e2"
      },
      "source": [
        "!python run_qa.py \\\n",
        "  --model_name_or_path distilbert-base-cased-distilled-squad \\\n",
        "  --dataset_name squad \\\n",
        "  --train_file ./data/squad/train-v1.1.json \\\n",
        "  --validation_file ./data/squad/sample1k-HCVerifyAll.json \\\n",
        "  --do_eval \\\n",
        "  --per_device_train_batch_size 12 \\\n",
        "  --learning_rate 3e-5 \\\n",
        "  --max_seq_length 384 \\\n",
        "  --doc_stride 128 \\\n",
        "  --output_dir ./tmp/debug_squad_distilled/ \\\n",
        "  --overwrite_output_dir"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-01-19 06:18:52.451962: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "01/19/2021 06:18:57 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
            "01/19/2021 06:18:57 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=./tmp/debug_squad_distilled/, overwrite_output_dir=True, do_train=False, do_eval=True, do_predict=False, evaluation_strategy=EvaluationStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=12, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=3e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_steps=0, logging_dir=runs/Jan19_06-18-57_6ffe1b88c389, logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=./tmp/debug_squad_distilled/, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, _n_gpu=0)\n",
            "Using custom data configuration default\n",
            "Downloading and preparing dataset squad/default-549ec43428507c2c (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/squad/default-549ec43428507c2c/0.0.0/f5babbce8c0f200bb625d7847c74110b6da7f1c2815b940aff2c6da4459f5a83...\n",
            "Dataset squad downloaded and prepared to /root/.cache/huggingface/datasets/squad/default-549ec43428507c2c/0.0.0/f5babbce8c0f200bb625d7847c74110b6da7f1c2815b940aff2c6da4459f5a83. Subsequent calls will reuse this data.\n",
            "https://huggingface.co/distilbert-base-cased-distilled-squad/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp1juhaidk\n",
            "Downloading: 100% 473/473 [00:00<00:00, 298kB/s]\n",
            "storing https://huggingface.co/distilbert-base-cased-distilled-squad/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/81e8dfe090123eff18dd06533ead3ae407b82e30834c50c7c82c2305ce3ace12.ca0305b1f128274fa0c6e4859d1c1477d0e34a20be25da95eea888b30ece9cf3\n",
            "creating metadata file for /root/.cache/huggingface/transformers/81e8dfe090123eff18dd06533ead3ae407b82e30834c50c7c82c2305ce3ace12.ca0305b1f128274fa0c6e4859d1c1477d0e34a20be25da95eea888b30ece9cf3\n",
            "loading configuration file https://huggingface.co/distilbert-base-cased-distilled-squad/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/81e8dfe090123eff18dd06533ead3ae407b82e30834c50c7c82c2305ce3ace12.ca0305b1f128274fa0c6e4859d1c1477d0e34a20be25da95eea888b30ece9cf3\n",
            "Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForQuestionAnswering\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": true,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.2.1\",\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/distilbert-base-cased-distilled-squad/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/81e8dfe090123eff18dd06533ead3ae407b82e30834c50c7c82c2305ce3ace12.ca0305b1f128274fa0c6e4859d1c1477d0e34a20be25da95eea888b30ece9cf3\n",
            "Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForQuestionAnswering\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": true,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.2.1\",\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "https://huggingface.co/bert-large-cased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpkem8y9cs\n",
            "Downloading: 100% 213k/213k [00:00<00:00, 1.89MB/s]\n",
            "storing https://huggingface.co/bert-large-cased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/c9961ea5b7e8ad58701728c45f4d225f70b19aa59745121e5a96c8a44efca4c8.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
            "creating metadata file for /root/.cache/huggingface/transformers/c9961ea5b7e8ad58701728c45f4d225f70b19aa59745121e5a96c8a44efca4c8.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
            "https://huggingface.co/bert-large-cased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpjlgigr6o\n",
            "Downloading: 100% 436k/436k [00:00<00:00, 3.11MB/s]\n",
            "storing https://huggingface.co/bert-large-cased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/75be22d7750034989358861e325977feda47740e1c3f8a4dc1cb73570aad843e.2b9a196704f2f183fe3f4b48d6e662dba8203fdcb3346bfa896831378edf6f97\n",
            "creating metadata file for /root/.cache/huggingface/transformers/75be22d7750034989358861e325977feda47740e1c3f8a4dc1cb73570aad843e.2b9a196704f2f183fe3f4b48d6e662dba8203fdcb3346bfa896831378edf6f97\n",
            "loading file https://huggingface.co/bert-large-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/c9961ea5b7e8ad58701728c45f4d225f70b19aa59745121e5a96c8a44efca4c8.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
            "loading file https://huggingface.co/bert-large-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/75be22d7750034989358861e325977feda47740e1c3f8a4dc1cb73570aad843e.2b9a196704f2f183fe3f4b48d6e662dba8203fdcb3346bfa896831378edf6f97\n",
            "https://huggingface.co/distilbert-base-cased-distilled-squad/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmphy5anuml\n",
            "Downloading: 100% 261M/261M [00:04<00:00, 60.2MB/s]\n",
            "storing https://huggingface.co/distilbert-base-cased-distilled-squad/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/02f483764e26e1acfddfcd6e4879785f2908b2806a962d01b888bfe2b988075b.bd96a9432b167ab5e2b086cf0b688ca3c43c027091377eddfcfd39bdde851c35\n",
            "creating metadata file for /root/.cache/huggingface/transformers/02f483764e26e1acfddfcd6e4879785f2908b2806a962d01b888bfe2b988075b.bd96a9432b167ab5e2b086cf0b688ca3c43c027091377eddfcfd39bdde851c35\n",
            "loading weights file https://huggingface.co/distilbert-base-cased-distilled-squad/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/02f483764e26e1acfddfcd6e4879785f2908b2806a962d01b888bfe2b988075b.bd96a9432b167ab5e2b086cf0b688ca3c43c027091377eddfcfd39bdde851c35\n",
            "All model checkpoint weights were used when initializing DistilBertForQuestionAnswering.\n",
            "\n",
            "All the weights of DistilBertForQuestionAnswering were initialized from the model checkpoint at distilbert-base-cased-distilled-squad.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForQuestionAnswering for predictions without further training.\n",
            "100% 4/4 [00:08<00:00,  2.10s/ba]\n",
            "Downloading: 4.02kB [00:00, 2.43MB/s]       \n",
            "Downloading: 3.35kB [00:00, 2.07MB/s]       \n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
            "01/19/2021 06:19:30 - INFO - __main__ -   *** Evaluate ***\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3702\n",
            "  Batch size = 8\n",
            "100% 463/463 [2:20:11<00:00, 16.90s/it]01/19/2021 08:40:04 - INFO - utils_qa -   Post-processing 3560 example predictions split into 3702 features.\n",
            "\n",
            "  0% 0/3560 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 39/3560 [00:00<00:09, 384.72it/s]\u001b[A\n",
            "  2% 84/3560 [00:00<00:08, 400.77it/s]\u001b[A\n",
            "  3% 113/3560 [00:00<00:09, 357.86it/s]\u001b[A\n",
            "  4% 137/3560 [00:00<00:11, 306.13it/s]\u001b[A\n",
            "  5% 178/3560 [00:00<00:10, 330.16it/s]\u001b[A\n",
            "  6% 211/3560 [00:00<00:10, 328.69it/s]\u001b[A\n",
            "  7% 247/3560 [00:00<00:09, 335.74it/s]\u001b[A\n",
            "  8% 287/3560 [00:00<00:09, 351.84it/s]\u001b[A\n",
            "  9% 321/3560 [00:00<00:09, 347.79it/s]\u001b[A\n",
            " 10% 356/3560 [00:01<00:09, 345.58it/s]\u001b[A\n",
            " 11% 391/3560 [00:01<00:09, 345.72it/s]\u001b[A\n",
            " 12% 427/3560 [00:01<00:09, 347.54it/s]\u001b[A\n",
            " 13% 462/3560 [00:01<00:09, 333.13it/s]\u001b[A\n",
            " 14% 498/3560 [00:01<00:09, 338.99it/s]\u001b[A\n",
            " 15% 537/3560 [00:01<00:08, 351.60it/s]\u001b[A\n",
            " 16% 579/3560 [00:01<00:08, 367.95it/s]\u001b[A\n",
            " 17% 617/3560 [00:01<00:08, 358.45it/s]\u001b[A\n",
            " 19% 659/3560 [00:01<00:07, 372.73it/s]\u001b[A\n",
            " 20% 698/3560 [00:01<00:07, 377.02it/s]\u001b[A\n",
            " 21% 736/3560 [00:02<00:07, 376.44it/s]\u001b[A\n",
            " 22% 776/3560 [00:02<00:07, 379.79it/s]\u001b[A\n",
            " 23% 815/3560 [00:02<00:07, 346.71it/s]\u001b[A\n",
            " 24% 851/3560 [00:02<00:08, 330.52it/s]\u001b[A\n",
            " 25% 887/3560 [00:02<00:07, 336.56it/s]\u001b[A\n",
            " 26% 930/3560 [00:02<00:07, 358.86it/s]\u001b[A\n",
            " 27% 967/3560 [00:02<00:07, 354.44it/s]\u001b[A\n",
            " 28% 1003/3560 [00:02<00:07, 341.37it/s]\u001b[A\n",
            " 29% 1038/3560 [00:02<00:07, 334.86it/s]\u001b[A\n",
            " 30% 1075/3560 [00:03<00:07, 344.09it/s]\u001b[A\n",
            " 31% 1110/3560 [00:03<00:07, 340.69it/s]\u001b[A\n",
            " 32% 1145/3560 [00:03<00:07, 338.72it/s]\u001b[A\n",
            " 33% 1181/3560 [00:03<00:06, 342.86it/s]\u001b[A\n",
            " 34% 1216/3560 [00:03<00:06, 342.40it/s]\u001b[A\n",
            " 35% 1251/3560 [00:03<00:06, 338.96it/s]\u001b[A\n",
            " 36% 1285/3560 [00:03<00:07, 312.96it/s]\u001b[A\n",
            " 37% 1322/3560 [00:03<00:06, 324.90it/s]\u001b[A\n",
            " 38% 1355/3560 [00:03<00:06, 325.14it/s]\u001b[A\n",
            " 39% 1388/3560 [00:04<00:07, 275.36it/s]\u001b[A\n",
            " 40% 1418/3560 [00:04<00:09, 231.22it/s]\u001b[A\n",
            " 41% 1444/3560 [00:04<00:11, 188.57it/s]\u001b[A\n",
            " 41% 1466/3560 [00:04<00:12, 171.37it/s]\u001b[A\n",
            " 42% 1500/3560 [00:04<00:10, 200.57it/s]\u001b[A\n",
            " 43% 1533/3560 [00:04<00:08, 226.25it/s]\u001b[A\n",
            " 44% 1567/3560 [00:04<00:07, 250.74it/s]\u001b[A\n",
            " 45% 1596/3560 [00:05<00:07, 259.37it/s]\u001b[A\n",
            " 46% 1634/3560 [00:05<00:06, 286.44it/s]\u001b[A\n",
            " 47% 1669/3560 [00:05<00:06, 302.94it/s]\u001b[A\n",
            " 48% 1702/3560 [00:05<00:06, 301.97it/s]\u001b[A\n",
            " 49% 1734/3560 [00:05<00:06, 303.16it/s]\u001b[A\n",
            " 50% 1766/3560 [00:05<00:05, 302.97it/s]\u001b[A\n",
            " 51% 1799/3560 [00:05<00:05, 308.66it/s]\u001b[A\n",
            " 51% 1831/3560 [00:05<00:05, 311.87it/s]\u001b[A\n",
            " 52% 1863/3560 [00:05<00:05, 313.80it/s]\u001b[A\n",
            " 53% 1896/3560 [00:05<00:05, 315.96it/s]\u001b[A\n",
            " 54% 1930/3560 [00:06<00:05, 321.40it/s]\u001b[A\n",
            " 55% 1965/3560 [00:06<00:04, 328.93it/s]\u001b[A\n",
            " 56% 2000/3560 [00:06<00:04, 334.46it/s]\u001b[A\n",
            " 57% 2034/3560 [00:06<00:04, 322.85it/s]\u001b[A\n",
            " 58% 2067/3560 [00:06<00:04, 323.88it/s]\u001b[A\n",
            " 59% 2100/3560 [00:06<00:04, 315.03it/s]\u001b[A\n",
            " 60% 2132/3560 [00:06<00:04, 310.95it/s]\u001b[A\n",
            " 61% 2168/3560 [00:06<00:04, 323.56it/s]\u001b[A\n",
            " 62% 2201/3560 [00:06<00:04, 312.46it/s]\u001b[A\n",
            " 63% 2234/3560 [00:06<00:04, 315.37it/s]\u001b[A\n",
            " 64% 2268/3560 [00:07<00:04, 320.70it/s]\u001b[A\n",
            " 65% 2301/3560 [00:07<00:03, 320.07it/s]\u001b[A\n",
            " 66% 2334/3560 [00:07<00:03, 321.69it/s]\u001b[A\n",
            " 67% 2369/3560 [00:07<00:03, 327.80it/s]\u001b[A\n",
            " 68% 2407/3560 [00:07<00:03, 340.71it/s]\u001b[A\n",
            " 69% 2444/3560 [00:07<00:03, 346.66it/s]\u001b[A\n",
            " 70% 2479/3560 [00:07<00:03, 339.10it/s]\u001b[A\n",
            " 71% 2514/3560 [00:07<00:03, 322.30it/s]\u001b[A\n",
            " 72% 2548/3560 [00:07<00:03, 325.44it/s]\u001b[A\n",
            " 73% 2584/3560 [00:08<00:02, 334.75it/s]\u001b[A\n",
            " 74% 2618/3560 [00:08<00:02, 321.75it/s]\u001b[A\n",
            " 74% 2651/3560 [00:08<00:02, 324.00it/s]\u001b[A\n",
            " 75% 2684/3560 [00:08<00:03, 287.20it/s]\u001b[A\n",
            " 76% 2718/3560 [00:08<00:02, 301.12it/s]\u001b[A\n",
            " 77% 2755/3560 [00:08<00:02, 316.89it/s]\u001b[A\n",
            " 78% 2788/3560 [00:08<00:02, 314.83it/s]\u001b[A\n",
            " 79% 2821/3560 [00:08<00:02, 301.47it/s]\u001b[A\n",
            " 80% 2857/3560 [00:08<00:02, 316.67it/s]\u001b[A\n",
            " 81% 2891/3560 [00:09<00:02, 321.82it/s]\u001b[A\n",
            " 82% 2924/3560 [00:09<00:01, 323.55it/s]\u001b[A\n",
            " 83% 2961/3560 [00:09<00:01, 335.93it/s]\u001b[A\n",
            " 84% 2996/3560 [00:09<00:01, 338.73it/s]\u001b[A\n",
            " 85% 3031/3560 [00:09<00:01, 340.50it/s]\u001b[A\n",
            " 86% 3067/3560 [00:09<00:01, 344.71it/s]\u001b[A\n",
            " 87% 3103/3560 [00:09<00:01, 346.17it/s]\u001b[A\n",
            " 88% 3138/3560 [00:09<00:01, 345.77it/s]\u001b[A\n",
            " 89% 3173/3560 [00:09<00:01, 338.96it/s]\u001b[A\n",
            " 90% 3210/3560 [00:09<00:01, 346.04it/s]\u001b[A\n",
            " 91% 3245/3560 [00:10<00:00, 331.08it/s]\u001b[A\n",
            " 92% 3279/3560 [00:10<00:00, 324.88it/s]\u001b[A\n",
            " 93% 3316/3560 [00:10<00:00, 336.77it/s]\u001b[A\n",
            " 94% 3350/3560 [00:10<00:00, 331.26it/s]\u001b[A\n",
            " 95% 3384/3560 [00:10<00:00, 311.59it/s]\u001b[A\n",
            " 96% 3417/3560 [00:10<00:00, 315.15it/s]\u001b[A\n",
            " 97% 3449/3560 [00:10<00:00, 311.04it/s]\u001b[A\n",
            " 98% 3481/3560 [00:10<00:00, 311.28it/s]\u001b[A\n",
            " 99% 3516/3560 [00:10<00:00, 321.86it/s]\u001b[A\n",
            "100% 3560/3560 [00:11<00:00, 322.54it/s]\n",
            "01/19/2021 08:40:15 - INFO - utils_qa -   Saving predictions to ./tmp/debug_squad_distilled/predictions.json.\n",
            "01/19/2021 08:40:15 - INFO - utils_qa -   Saving nbest_preds to ./tmp/debug_squad_distilled/nbest_predictions.json.\n",
            "100% 463/463 [2:20:28<00:00, 18.20s/it]\n",
            "01/19/2021 08:40:17 - INFO - __main__ -   ***** Eval results *****\n",
            "01/19/2021 08:40:17 - INFO - __main__ -     exact_match = 55.337078651685395\n",
            "01/19/2021 08:40:17 - INFO - __main__ -     f1 = 61.78296650484933\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qs5FqlRvto-g"
      },
      "source": [
        "Eval_squad results for Distilled on HCVeridyAll - {\"orig_exact_match\": 80.2, \"orig_f1\": 86.71694180566296, \"adv_exact_match\": 43.6, \"adv_f1\": 48.60186557659448}\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2wTkpuQzYLw",
        "outputId": "e3a10919-d578-4fce-bfa2-87d6fe96339e"
      },
      "source": [
        "!python run_qa.py \\\n",
        "  --model_name_or_path distilbert-base-cased-distilled-squad \\\n",
        "  --dataset_name squad \\\n",
        "  --train_file ./data/squad/train-v1.1.json \\\n",
        "  --validation_file ./data/squad/sample1k-HCVerifySample.json \\\n",
        "  --do_eval \\\n",
        "  --per_device_train_batch_size 12 \\\n",
        "  --learning_rate 3e-5 \\\n",
        "  --max_seq_length 384 \\\n",
        "  --doc_stride 128 \\\n",
        "  --output_dir ./tmp/debug_squad_distilled_sample/ \\\n",
        "  --overwrite_output_dir"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-01-19 10:31:12.586566: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "01/19/2021 10:31:15 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
            "01/19/2021 10:31:15 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=./tmp/debug_squad_distilled_sample/, overwrite_output_dir=True, do_train=False, do_eval=True, do_predict=False, evaluation_strategy=EvaluationStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=12, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=3e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_steps=0, logging_dir=runs/Jan19_10-31-15_6ffe1b88c389, logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=./tmp/debug_squad_distilled_sample/, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, _n_gpu=0)\n",
            "Using custom data configuration default\n",
            "Downloading and preparing dataset squad/default-1f0ff591794b591b (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/squad/default-1f0ff591794b591b/0.0.0/f5babbce8c0f200bb625d7847c74110b6da7f1c2815b940aff2c6da4459f5a83...\n",
            "Dataset squad downloaded and prepared to /root/.cache/huggingface/datasets/squad/default-1f0ff591794b591b/0.0.0/f5babbce8c0f200bb625d7847c74110b6da7f1c2815b940aff2c6da4459f5a83. Subsequent calls will reuse this data.\n",
            "loading configuration file https://huggingface.co/distilbert-base-cased-distilled-squad/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/81e8dfe090123eff18dd06533ead3ae407b82e30834c50c7c82c2305ce3ace12.ca0305b1f128274fa0c6e4859d1c1477d0e34a20be25da95eea888b30ece9cf3\n",
            "Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForQuestionAnswering\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": true,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.2.1\",\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/distilbert-base-cased-distilled-squad/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/81e8dfe090123eff18dd06533ead3ae407b82e30834c50c7c82c2305ce3ace12.ca0305b1f128274fa0c6e4859d1c1477d0e34a20be25da95eea888b30ece9cf3\n",
            "Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForQuestionAnswering\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": true,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.2.1\",\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/bert-large-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/c9961ea5b7e8ad58701728c45f4d225f70b19aa59745121e5a96c8a44efca4c8.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
            "loading file https://huggingface.co/bert-large-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/75be22d7750034989358861e325977feda47740e1c3f8a4dc1cb73570aad843e.2b9a196704f2f183fe3f4b48d6e662dba8203fdcb3346bfa896831378edf6f97\n",
            "loading weights file https://huggingface.co/distilbert-base-cased-distilled-squad/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/02f483764e26e1acfddfcd6e4879785f2908b2806a962d01b888bfe2b988075b.bd96a9432b167ab5e2b086cf0b688ca3c43c027091377eddfcfd39bdde851c35\n",
            "All model checkpoint weights were used when initializing DistilBertForQuestionAnswering.\n",
            "\n",
            "All the weights of DistilBertForQuestionAnswering were initialized from the model checkpoint at distilbert-base-cased-distilled-squad.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForQuestionAnswering for predictions without further training.\n",
            "100% 2/2 [00:04<00:00,  2.18s/ba]\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
            "01/19/2021 10:31:32 - INFO - __main__ -   *** Evaluate ***\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1851\n",
            "  Batch size = 8\n",
            "100% 232/232 [1:10:30<00:00, 14.78s/it]01/19/2021 11:42:24 - INFO - utils_qa -   Post-processing 1787 example predictions split into 1851 features.\n",
            "\n",
            "  0% 0/1787 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 38/1787 [00:00<00:04, 378.80it/s]\u001b[A\n",
            "  4% 65/1787 [00:00<00:05, 336.54it/s]\u001b[A\n",
            "  6% 105/1787 [00:00<00:04, 350.38it/s]\u001b[A\n",
            "  8% 146/1787 [00:00<00:04, 364.60it/s]\u001b[A\n",
            " 10% 180/1787 [00:00<00:04, 356.25it/s]\u001b[A\n",
            " 12% 214/1787 [00:00<00:04, 349.85it/s]\u001b[A\n",
            " 14% 247/1787 [00:00<00:04, 342.92it/s]\u001b[A\n",
            " 16% 283/1787 [00:00<00:04, 347.37it/s]\u001b[A\n",
            " 18% 319/1787 [00:00<00:04, 348.39it/s]\u001b[A\n",
            " 20% 356/1787 [00:01<00:04, 352.88it/s]\u001b[A\n",
            " 22% 392/1787 [00:01<00:03, 354.66it/s]\u001b[A\n",
            " 24% 427/1787 [00:01<00:04, 332.49it/s]\u001b[A\n",
            " 26% 466/1787 [00:01<00:03, 347.60it/s]\u001b[A\n",
            " 28% 504/1787 [00:01<00:03, 356.48it/s]\u001b[A\n",
            " 30% 540/1787 [00:01<00:03, 346.66it/s]\u001b[A\n",
            " 32% 575/1787 [00:01<00:03, 347.11it/s]\u001b[A\n",
            " 34% 611/1787 [00:01<00:03, 350.21it/s]\u001b[A\n",
            " 36% 647/1787 [00:01<00:03, 345.32it/s]\u001b[A\n",
            " 38% 682/1787 [00:01<00:03, 340.00it/s]\u001b[A\n",
            " 40% 717/1787 [00:02<00:04, 266.56it/s]\u001b[A\n",
            " 42% 747/1787 [00:02<00:04, 252.44it/s]\u001b[A\n",
            " 44% 782/1787 [00:02<00:03, 273.73it/s]\u001b[A\n",
            " 45% 813/1787 [00:02<00:03, 281.81it/s]\u001b[A\n",
            " 48% 850/1787 [00:02<00:03, 302.96it/s]\u001b[A\n",
            " 49% 884/1787 [00:02<00:02, 312.55it/s]\u001b[A\n",
            " 51% 918/1787 [00:02<00:02, 319.51it/s]\u001b[A\n",
            " 53% 952/1787 [00:02<00:02, 323.10it/s]\u001b[A\n",
            " 55% 985/1787 [00:03<00:02, 312.66it/s]\u001b[A\n",
            " 57% 1017/1787 [00:03<00:02, 310.12it/s]\u001b[A\n",
            " 59% 1052/1787 [00:03<00:02, 320.35it/s]\u001b[A\n",
            " 61% 1085/1787 [00:03<00:02, 308.15it/s]\u001b[A\n",
            " 63% 1123/1787 [00:03<00:02, 324.68it/s]\u001b[A\n",
            " 65% 1157/1787 [00:03<00:01, 327.22it/s]\u001b[A\n",
            " 67% 1191/1787 [00:03<00:01, 327.53it/s]\u001b[A\n",
            " 69% 1227/1787 [00:03<00:01, 333.90it/s]\u001b[A\n",
            " 71% 1261/1787 [00:03<00:01, 326.76it/s]\u001b[A\n",
            " 72% 1294/1787 [00:03<00:01, 322.31it/s]\u001b[A\n",
            " 74% 1327/1787 [00:04<00:01, 309.77it/s]\u001b[A\n",
            " 76% 1359/1787 [00:04<00:01, 306.05it/s]\u001b[A\n",
            " 78% 1390/1787 [00:04<00:01, 295.95it/s]\u001b[A\n",
            " 79% 1420/1787 [00:04<00:01, 294.73it/s]\u001b[A\n",
            " 81% 1454/1787 [00:04<00:01, 306.69it/s]\u001b[A\n",
            " 83% 1491/1787 [00:04<00:00, 320.44it/s]\u001b[A\n",
            " 85% 1524/1787 [00:04<00:00, 311.65it/s]\u001b[A\n",
            " 87% 1557/1787 [00:04<00:00, 316.15it/s]\u001b[A\n",
            " 89% 1594/1787 [00:04<00:00, 328.04it/s]\u001b[A\n",
            " 91% 1628/1787 [00:05<00:00, 327.91it/s]\u001b[A\n",
            " 93% 1665/1787 [00:05<00:00, 335.95it/s]\u001b[A\n",
            " 95% 1699/1787 [00:05<00:00, 327.52it/s]\u001b[A\n",
            " 97% 1736/1787 [00:05<00:00, 337.03it/s]\u001b[A\n",
            "100% 1787/1787 [00:05<00:00, 325.44it/s]\n",
            "01/19/2021 11:42:29 - INFO - utils_qa -   Saving predictions to ./tmp/debug_squad_distilled_sample/predictions.json.\n",
            "01/19/2021 11:42:29 - INFO - utils_qa -   Saving nbest_preds to ./tmp/debug_squad_distilled_sample/nbest_predictions.json.\n",
            "100% 232/232 [1:10:39<00:00, 18.27s/it]\n",
            "01/19/2021 11:42:30 - INFO - __main__ -   ***** Eval results *****\n",
            "01/19/2021 11:42:30 - INFO - __main__ -     exact_match = 64.85730274202574\n",
            "01/19/2021 11:42:30 - INFO - __main__ -     f1 = 71.15319538428102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdZ3EBNnt6c6"
      },
      "source": [
        "Eval_squad results for Distilled on HCVeridySample - {\"orig_exact_match\": 80.2, \"orig_f1\": 86.71694180566296, \"adv_exact_match\": 52.8, \"adv_f1\": 59.08220651343544}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84UMfo0Fkmnc"
      },
      "source": [
        "Running on Roberta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4_F9c4Jkovu",
        "outputId": "88dfb014-e854-473b-932a-b1ac22231c16"
      },
      "source": [
        "!python run_qa.py \\\n",
        "  --model_name_or_path csarron/roberta-large-squad-v1 \\\n",
        "  --dataset_name squad \\\n",
        "  --train_file ./data/squad/train-v1.1.json \\\n",
        "  --validation_file ./data/squad/sample1k-HCVerifySample.json \\\n",
        "  --do_eval \\\n",
        "  --per_device_train_batch_size 12 \\\n",
        "  --learning_rate 3e-5 \\\n",
        "  --max_seq_length 384 \\\n",
        "  --doc_stride 128 \\\n",
        "  --output_dir ./tmp/debug_squad_roberta_sample/ \\\n",
        "  --overwrite_output_dir"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-01-19 19:08:07.616056: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "01/19/2021 19:08:13 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
            "01/19/2021 19:08:13 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=./tmp/debug_squad_roberta_sample/, overwrite_output_dir=True, do_train=False, do_eval=True, do_predict=False, evaluation_strategy=EvaluationStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=12, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=3e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_steps=0, logging_dir=runs/Jan19_19-08-13_fe3b0d9565f3, logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=./tmp/debug_squad_roberta_sample/, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, _n_gpu=0)\n",
            "Using custom data configuration default\n",
            "Downloading and preparing dataset squad/default-33171079d281940c (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/squad/default-33171079d281940c/0.0.0/f5babbce8c0f200bb625d7847c74110b6da7f1c2815b940aff2c6da4459f5a83...\n",
            "Dataset squad downloaded and prepared to /root/.cache/huggingface/datasets/squad/default-33171079d281940c/0.0.0/f5babbce8c0f200bb625d7847c74110b6da7f1c2815b940aff2c6da4459f5a83. Subsequent calls will reuse this data.\n",
            "https://huggingface.co/csarron/roberta-large-squad-v1/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpr8i_agj7\n",
            "Downloading: 100% 526/526 [00:00<00:00, 363kB/s]\n",
            "storing https://huggingface.co/csarron/roberta-large-squad-v1/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/0fc041ca661ebb7473123e9722819c48e27f9a83aedf13e1150435275cc21bdd.e59dc72a749f921962e2e724ff292de7eb1b9902723153972b2069e8847ff4f0\n",
            "creating metadata file for /root/.cache/huggingface/transformers/0fc041ca661ebb7473123e9722819c48e27f9a83aedf13e1150435275cc21bdd.e59dc72a749f921962e2e724ff292de7eb1b9902723153972b2069e8847ff4f0\n",
            "loading configuration file https://huggingface.co/csarron/roberta-large-squad-v1/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/0fc041ca661ebb7473123e9722819c48e27f9a83aedf13e1150435275cc21bdd.e59dc72a749f921962e2e724ff292de7eb1b9902723153972b2069e8847ff4f0\n",
            "Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForQuestionAnswering\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.2.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/csarron/roberta-large-squad-v1/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/0fc041ca661ebb7473123e9722819c48e27f9a83aedf13e1150435275cc21bdd.e59dc72a749f921962e2e724ff292de7eb1b9902723153972b2069e8847ff4f0\n",
            "Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForQuestionAnswering\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.2.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "Model name 'csarron/roberta-large-squad-v1' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'csarron/roberta-large-squad-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "https://huggingface.co/csarron/roberta-large-squad-v1/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp04pnfw4g\n",
            "Downloading: 100% 899k/899k [00:00<00:00, 5.44MB/s]\n",
            "storing https://huggingface.co/csarron/roberta-large-squad-v1/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/5e33c3e3fc394fe42529078e25b17308c41253999ae658832e1d00c1e35e2f23.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\n",
            "creating metadata file for /root/.cache/huggingface/transformers/5e33c3e3fc394fe42529078e25b17308c41253999ae658832e1d00c1e35e2f23.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\n",
            "https://huggingface.co/csarron/roberta-large-squad-v1/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp7w6_06z0\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 3.24MB/s]\n",
            "storing https://huggingface.co/csarron/roberta-large-squad-v1/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/48047eb6f8b799c467d0f97316aa9b6e520be6ace95983203d421834eb0c0b54.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "creating metadata file for /root/.cache/huggingface/transformers/48047eb6f8b799c467d0f97316aa9b6e520be6ace95983203d421834eb0c0b54.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "https://huggingface.co/csarron/roberta-large-squad-v1/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp7fwtmsts\n",
            "Downloading: 100% 772/772 [00:00<00:00, 556kB/s]\n",
            "storing https://huggingface.co/csarron/roberta-large-squad-v1/resolve/main/special_tokens_map.json in cache at /root/.cache/huggingface/transformers/030f51904149258d64958d23a91fbc76c0fe866b3a4c2605c337b5adbb330e3e.cb2244924ab24d706b02fd7fcedaea4531566537687a539ebb94db511fd122a0\n",
            "creating metadata file for /root/.cache/huggingface/transformers/030f51904149258d64958d23a91fbc76c0fe866b3a4c2605c337b5adbb330e3e.cb2244924ab24d706b02fd7fcedaea4531566537687a539ebb94db511fd122a0\n",
            "https://huggingface.co/csarron/roberta-large-squad-v1/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpr2lv9jc9\n",
            "Downloading: 100% 49.0/49.0 [00:00<00:00, 14.8kB/s]\n",
            "storing https://huggingface.co/csarron/roberta-large-squad-v1/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/d83af4fbe61613dfaf70774340a396b54325045622496964fb17caa959c6f04c.25d8d06fb0679146a3ed2a3463e3585380bff882fe6e1ebc497196e40dbbd7fa\n",
            "creating metadata file for /root/.cache/huggingface/transformers/d83af4fbe61613dfaf70774340a396b54325045622496964fb17caa959c6f04c.25d8d06fb0679146a3ed2a3463e3585380bff882fe6e1ebc497196e40dbbd7fa\n",
            "loading file https://huggingface.co/csarron/roberta-large-squad-v1/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/5e33c3e3fc394fe42529078e25b17308c41253999ae658832e1d00c1e35e2f23.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\n",
            "loading file https://huggingface.co/csarron/roberta-large-squad-v1/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/48047eb6f8b799c467d0f97316aa9b6e520be6ace95983203d421834eb0c0b54.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "loading file https://huggingface.co/csarron/roberta-large-squad-v1/resolve/main/tokenizer.json from cache at None\n",
            "loading file https://huggingface.co/csarron/roberta-large-squad-v1/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/csarron/roberta-large-squad-v1/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/030f51904149258d64958d23a91fbc76c0fe866b3a4c2605c337b5adbb330e3e.cb2244924ab24d706b02fd7fcedaea4531566537687a539ebb94db511fd122a0\n",
            "loading file https://huggingface.co/csarron/roberta-large-squad-v1/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/d83af4fbe61613dfaf70774340a396b54325045622496964fb17caa959c6f04c.25d8d06fb0679146a3ed2a3463e3585380bff882fe6e1ebc497196e40dbbd7fa\n",
            "https://huggingface.co/csarron/roberta-large-squad-v1/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp_unabv5d\n",
            "Downloading: 100% 1.42G/1.42G [00:27<00:00, 51.9MB/s]\n",
            "storing https://huggingface.co/csarron/roberta-large-squad-v1/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/935ac7e3a963d7bbb378084e7fd258dff35107d1f2adf0330552dccd9d042b67.dcb0046418df96c9fb3968477de7cd19a4f6ccb9b4796bca32743ccf081b9859\n",
            "creating metadata file for /root/.cache/huggingface/transformers/935ac7e3a963d7bbb378084e7fd258dff35107d1f2adf0330552dccd9d042b67.dcb0046418df96c9fb3968477de7cd19a4f6ccb9b4796bca32743ccf081b9859\n",
            "loading weights file https://huggingface.co/csarron/roberta-large-squad-v1/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/935ac7e3a963d7bbb378084e7fd258dff35107d1f2adf0330552dccd9d042b67.dcb0046418df96c9fb3968477de7cd19a4f6ccb9b4796bca32743ccf081b9859\n",
            "All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
            "\n",
            "All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at csarron/roberta-large-squad-v1.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
            "100% 2/2 [00:04<00:00,  2.04s/ba]\n",
            "Downloading: 4.02kB [00:00, 2.11MB/s]       \n",
            "Downloading: 3.35kB [00:00, 2.13MB/s]       \n",
            "The following columns in the evaluation set don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
            "01/19/2021 19:09:13 - INFO - __main__ -   *** Evaluate ***\n",
            "The following columns in the evaluation set don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1838\n",
            "  Batch size = 8\n",
            "100% 230/230 [2:17:13<00:00, 33.14s/it]01/19/2021 21:27:04 - INFO - utils_qa -   Post-processing 1787 example predictions split into 1838 features.\n",
            "\n",
            "  0% 0/1787 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 50/1787 [00:00<00:03, 491.78it/s]\u001b[A\n",
            "  5% 95/1787 [00:00<00:03, 476.85it/s]\u001b[A\n",
            "  8% 146/1787 [00:00<00:03, 484.47it/s]\u001b[A\n",
            " 11% 191/1787 [00:00<00:03, 473.06it/s]\u001b[A\n",
            " 13% 236/1787 [00:00<00:03, 463.24it/s]\u001b[A\n",
            " 16% 284/1787 [00:00<00:03, 467.43it/s]\u001b[A\n",
            " 19% 340/1787 [00:00<00:02, 490.46it/s]\u001b[A\n",
            " 22% 387/1787 [00:00<00:02, 482.92it/s]\u001b[A\n",
            " 24% 433/1787 [00:00<00:02, 452.31it/s]\u001b[A\n",
            " 27% 488/1787 [00:01<00:02, 476.11it/s]\u001b[A\n",
            " 30% 535/1787 [00:01<00:02, 473.43it/s]\u001b[A\n",
            " 33% 582/1787 [00:01<00:02, 470.51it/s]\u001b[A\n",
            " 35% 629/1787 [00:01<00:02, 469.45it/s]\u001b[A\n",
            " 38% 677/1787 [00:01<00:02, 471.02it/s]\u001b[A\n",
            " 41% 724/1787 [00:01<00:02, 373.73it/s]\u001b[A\n",
            " 43% 769/1787 [00:01<00:02, 393.61it/s]\u001b[A\n",
            " 46% 816/1787 [00:01<00:02, 413.47it/s]\u001b[A\n",
            " 48% 860/1787 [00:01<00:02, 412.27it/s]\u001b[A\n",
            " 51% 908/1787 [00:02<00:02, 428.19it/s]\u001b[A\n",
            " 53% 954/1787 [00:02<00:01, 434.91it/s]\u001b[A\n",
            " 56% 999/1787 [00:02<00:01, 438.10it/s]\u001b[A\n",
            " 59% 1049/1787 [00:02<00:01, 452.67it/s]\u001b[A\n",
            " 61% 1095/1787 [00:02<00:01, 439.32it/s]\u001b[A\n",
            " 64% 1141/1787 [00:02<00:01, 445.00it/s]\u001b[A\n",
            " 67% 1190/1787 [00:02<00:01, 456.82it/s]\u001b[A\n",
            " 69% 1237/1787 [00:02<00:01, 458.69it/s]\u001b[A\n",
            " 72% 1284/1787 [00:02<00:01, 455.96it/s]\u001b[A\n",
            " 74% 1330/1787 [00:02<00:01, 432.09it/s]\u001b[A\n",
            " 77% 1379/1787 [00:03<00:00, 446.01it/s]\u001b[A\n",
            " 80% 1424/1787 [00:03<00:00, 428.84it/s]\u001b[A\n",
            " 82% 1473/1787 [00:03<00:00, 444.89it/s]\u001b[A\n",
            " 85% 1520/1787 [00:03<00:00, 452.13it/s]\u001b[A\n",
            " 88% 1566/1787 [00:03<00:00, 447.66it/s]\u001b[A\n",
            " 90% 1613/1787 [00:03<00:00, 451.48it/s]\u001b[A\n",
            " 93% 1663/1787 [00:03<00:00, 463.09it/s]\u001b[A\n",
            " 96% 1710/1787 [00:03<00:00, 464.17it/s]\u001b[A\n",
            "100% 1787/1787 [00:03<00:00, 450.36it/s]\n",
            "01/19/2021 21:27:08 - INFO - utils_qa -   Saving predictions to ./tmp/debug_squad_roberta_sample/predictions.json.\n",
            "01/19/2021 21:27:08 - INFO - utils_qa -   Saving nbest_preds to ./tmp/debug_squad_roberta_sample/nbest_predictions.json.\n",
            "100% 230/230 [2:17:20<00:00, 35.83s/it]\n",
            "01/19/2021 21:27:09 - INFO - __main__ -   ***** Eval results *****\n",
            "01/19/2021 21:27:09 - INFO - __main__ -     exact_match = 80.80581980973699\n",
            "01/19/2021 21:27:09 - INFO - __main__ -     f1 = 87.60628287239592\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMR6738DjYeQ",
        "outputId": "2ec4d4f8-ad63-452b-bbd7-6a16ac97ab2a"
      },
      "source": [
        "!python run_qa.py \\\r\n",
        "  --model_name_or_path csarron/roberta-large-squad-v1 \\\r\n",
        "  --dataset_name squad \\\r\n",
        "  --train_file ./data/squad/train-v1.1.json \\\r\n",
        "  --validation_file ./data/squad/sample1k-HCVerifyAll.json \\\r\n",
        "  --do_eval \\\r\n",
        "  --per_device_train_batch_size 12 \\\r\n",
        "  --learning_rate 3e-5 \\\r\n",
        "  --max_seq_length 384 \\\r\n",
        "  --doc_stride 128 \\\r\n",
        "  --output_dir ./tmp/debug_squad_roberta_sampleAll/ \\\r\n",
        "  --overwrite_output_dir"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-01-19 21:29:55.462160: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "01/19/2021 21:29:57 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
            "01/19/2021 21:29:57 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=./tmp/debug_squad_roberta_sampleAll/, overwrite_output_dir=True, do_train=False, do_eval=True, do_predict=False, evaluation_strategy=EvaluationStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=12, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=3e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_steps=0, logging_dir=runs/Jan19_21-29-57_fe3b0d9565f3, logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=./tmp/debug_squad_roberta_sampleAll/, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, _n_gpu=0)\n",
            "Using custom data configuration default\n",
            "Downloading and preparing dataset squad/default-549ec43428507c2c (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/squad/default-549ec43428507c2c/0.0.0/f5babbce8c0f200bb625d7847c74110b6da7f1c2815b940aff2c6da4459f5a83...\n",
            "Dataset squad downloaded and prepared to /root/.cache/huggingface/datasets/squad/default-549ec43428507c2c/0.0.0/f5babbce8c0f200bb625d7847c74110b6da7f1c2815b940aff2c6da4459f5a83. Subsequent calls will reuse this data.\n",
            "loading configuration file https://huggingface.co/csarron/roberta-large-squad-v1/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/0fc041ca661ebb7473123e9722819c48e27f9a83aedf13e1150435275cc21bdd.e59dc72a749f921962e2e724ff292de7eb1b9902723153972b2069e8847ff4f0\n",
            "Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForQuestionAnswering\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.2.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/csarron/roberta-large-squad-v1/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/0fc041ca661ebb7473123e9722819c48e27f9a83aedf13e1150435275cc21bdd.e59dc72a749f921962e2e724ff292de7eb1b9902723153972b2069e8847ff4f0\n",
            "Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForQuestionAnswering\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.2.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "Model name 'csarron/roberta-large-squad-v1' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'csarron/roberta-large-squad-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "loading file https://huggingface.co/csarron/roberta-large-squad-v1/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/5e33c3e3fc394fe42529078e25b17308c41253999ae658832e1d00c1e35e2f23.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\n",
            "loading file https://huggingface.co/csarron/roberta-large-squad-v1/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/48047eb6f8b799c467d0f97316aa9b6e520be6ace95983203d421834eb0c0b54.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "loading file https://huggingface.co/csarron/roberta-large-squad-v1/resolve/main/tokenizer.json from cache at None\n",
            "loading file https://huggingface.co/csarron/roberta-large-squad-v1/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/csarron/roberta-large-squad-v1/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/030f51904149258d64958d23a91fbc76c0fe866b3a4c2605c337b5adbb330e3e.cb2244924ab24d706b02fd7fcedaea4531566537687a539ebb94db511fd122a0\n",
            "loading file https://huggingface.co/csarron/roberta-large-squad-v1/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/d83af4fbe61613dfaf70774340a396b54325045622496964fb17caa959c6f04c.25d8d06fb0679146a3ed2a3463e3585380bff882fe6e1ebc497196e40dbbd7fa\n",
            "loading weights file https://huggingface.co/csarron/roberta-large-squad-v1/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/935ac7e3a963d7bbb378084e7fd258dff35107d1f2adf0330552dccd9d042b67.dcb0046418df96c9fb3968477de7cd19a4f6ccb9b4796bca32743ccf081b9859\n",
            "All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
            "\n",
            "All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at csarron/roberta-large-squad-v1.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
            "100% 4/4 [00:07<00:00,  1.95s/ba]\n",
            "The following columns in the evaluation set don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
            "01/19/2021 21:30:26 - INFO - __main__ -   *** Evaluate ***\n",
            "The following columns in the evaluation set don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3669\n",
            "  Batch size = 8\n",
            "100% 459/459 [4:31:51<00:00, 31.65s/it]01/20/2021 02:02:57 - INFO - utils_qa -   Post-processing 3560 example predictions split into 3669 features.\n",
            "\n",
            "  0% 0/3560 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 50/3560 [00:00<00:07, 495.78it/s]\u001b[A\n",
            "  3% 105/3560 [00:00<00:06, 508.21it/s]\u001b[A\n",
            "  4% 138/3560 [00:00<00:07, 436.09it/s]\u001b[A\n",
            "  5% 191/3560 [00:00<00:07, 460.10it/s]\u001b[A\n",
            "  7% 243/3560 [00:00<00:06, 474.59it/s]\u001b[A\n",
            "  8% 296/3560 [00:00<00:06, 489.15it/s]\u001b[A\n",
            " 10% 341/3560 [00:00<00:07, 453.91it/s]\u001b[A\n",
            " 11% 384/3560 [00:00<00:07, 440.80it/s]\u001b[A\n",
            " 12% 430/3560 [00:00<00:07, 445.32it/s]\u001b[A\n",
            " 13% 474/3560 [00:01<00:07, 438.60it/s]\u001b[A\n",
            " 15% 522/3560 [00:01<00:06, 447.43it/s]\u001b[A\n",
            " 16% 573/3560 [00:01<00:06, 463.88it/s]\u001b[A\n",
            " 18% 626/3560 [00:01<00:06, 479.98it/s]\u001b[A\n",
            " 19% 677/3560 [00:01<00:05, 484.90it/s]\u001b[A\n",
            " 20% 726/3560 [00:01<00:05, 483.60it/s]\u001b[A\n",
            " 22% 777/3560 [00:01<00:05, 489.36it/s]\u001b[A\n",
            " 23% 826/3560 [00:01<00:05, 458.65it/s]\u001b[A\n",
            " 25% 874/3560 [00:01<00:05, 464.11it/s]\u001b[A\n",
            " 26% 927/3560 [00:01<00:05, 481.99it/s]\u001b[A\n",
            " 27% 976/3560 [00:02<00:05, 472.12it/s]\u001b[A\n",
            " 29% 1024/3560 [00:02<00:05, 469.87it/s]\u001b[A\n",
            " 30% 1073/3560 [00:02<00:05, 474.91it/s]\u001b[A\n",
            " 31% 1121/3560 [00:02<00:05, 453.04it/s]\u001b[A\n",
            " 33% 1169/3560 [00:02<00:05, 458.68it/s]\u001b[A\n",
            " 34% 1217/3560 [00:02<00:05, 462.97it/s]\u001b[A\n",
            " 36% 1264/3560 [00:02<00:05, 451.15it/s]\u001b[A\n",
            " 37% 1310/3560 [00:02<00:04, 452.95it/s]\u001b[A\n",
            " 38% 1356/3560 [00:02<00:04, 453.94it/s]\u001b[A\n",
            " 39% 1402/3560 [00:03<00:05, 391.33it/s]\u001b[A\n",
            " 41% 1443/3560 [00:03<00:06, 325.39it/s]\u001b[A\n",
            " 42% 1479/3560 [00:03<00:06, 301.77it/s]\u001b[A\n",
            " 43% 1528/3560 [00:03<00:05, 339.73it/s]\u001b[A\n",
            " 44% 1575/3560 [00:03<00:05, 369.69it/s]\u001b[A\n",
            " 46% 1620/3560 [00:03<00:04, 389.75it/s]\u001b[A\n",
            " 47% 1666/3560 [00:03<00:04, 404.49it/s]\u001b[A\n",
            " 48% 1710/3560 [00:03<00:04, 411.46it/s]\u001b[A\n",
            " 49% 1754/3560 [00:03<00:04, 419.48it/s]\u001b[A\n",
            " 51% 1800/3560 [00:04<00:04, 430.36it/s]\u001b[A\n",
            " 52% 1847/3560 [00:04<00:03, 439.43it/s]\u001b[A\n",
            " 53% 1892/3560 [00:04<00:03, 417.00it/s]\u001b[A\n",
            " 54% 1939/3560 [00:04<00:03, 430.94it/s]\u001b[A\n",
            " 56% 1987/3560 [00:04<00:03, 441.91it/s]\u001b[A\n",
            " 57% 2032/3560 [00:04<00:03, 440.29it/s]\u001b[A\n",
            " 58% 2077/3560 [00:04<00:03, 442.61it/s]\u001b[A\n",
            " 60% 2122/3560 [00:04<00:03, 440.74it/s]\u001b[A\n",
            " 61% 2168/3560 [00:04<00:03, 444.20it/s]\u001b[A\n",
            " 62% 2214/3560 [00:05<00:03, 448.55it/s]\u001b[A\n",
            " 64% 2261/3560 [00:05<00:02, 454.15it/s]\u001b[A\n",
            " 65% 2308/3560 [00:05<00:02, 456.36it/s]\u001b[A\n",
            " 66% 2355/3560 [00:05<00:02, 458.21it/s]\u001b[A\n",
            " 67% 2402/3560 [00:05<00:02, 459.35it/s]\u001b[A\n",
            " 69% 2453/3560 [00:05<00:02, 472.96it/s]\u001b[A\n",
            " 70% 2501/3560 [00:05<00:02, 453.02it/s]\u001b[A\n",
            " 72% 2548/3560 [00:05<00:02, 455.75it/s]\u001b[A\n",
            " 73% 2594/3560 [00:05<00:02, 444.31it/s]\u001b[A\n",
            " 74% 2642/3560 [00:05<00:02, 452.62it/s]\u001b[A\n",
            " 76% 2688/3560 [00:06<00:02, 414.72it/s]\u001b[A\n",
            " 77% 2735/3560 [00:06<00:01, 429.50it/s]\u001b[A\n",
            " 78% 2779/3560 [00:06<00:01, 427.21it/s]\u001b[A\n",
            " 79% 2823/3560 [00:06<00:01, 412.51it/s]\u001b[A\n",
            " 81% 2868/3560 [00:06<00:01, 423.01it/s]\u001b[A\n",
            " 82% 2913/3560 [00:06<00:01, 430.38it/s]\u001b[A\n",
            " 83% 2960/3560 [00:06<00:01, 441.01it/s]\u001b[A\n",
            " 84% 3006/3560 [00:06<00:01, 444.69it/s]\u001b[A\n",
            " 86% 3051/3560 [00:06<00:01, 438.39it/s]\u001b[A\n",
            " 87% 3097/3560 [00:07<00:01, 443.10it/s]\u001b[A\n",
            " 88% 3142/3560 [00:07<00:00, 444.13it/s]\u001b[A\n",
            " 90% 3189/3560 [00:07<00:00, 450.22it/s]\u001b[A\n",
            " 91% 3238/3560 [00:07<00:00, 461.29it/s]\u001b[A\n",
            " 92% 3285/3560 [00:07<00:00, 462.07it/s]\u001b[A\n",
            " 94% 3336/3560 [00:07<00:00, 472.86it/s]\u001b[A\n",
            " 95% 3384/3560 [00:07<00:00, 460.61it/s]\u001b[A\n",
            " 96% 3431/3560 [00:07<00:00, 447.42it/s]\u001b[A\n",
            " 98% 3477/3560 [00:07<00:00, 449.62it/s]\u001b[A\n",
            "100% 3560/3560 [00:08<00:00, 443.15it/s]\n",
            "01/20/2021 02:03:05 - INFO - utils_qa -   Saving predictions to ./tmp/debug_squad_roberta_sampleAll/predictions.json.\n",
            "01/20/2021 02:03:05 - INFO - utils_qa -   Saving nbest_preds to ./tmp/debug_squad_roberta_sampleAll/nbest_predictions.json.\n",
            "100% 459/459 [4:32:05<00:00, 35.57s/it]\n",
            "01/20/2021 02:03:07 - INFO - __main__ -   ***** Eval results *****\n",
            "01/20/2021 02:03:07 - INFO - __main__ -     exact_match = 76.48876404494382\n",
            "01/20/2021 02:03:07 - INFO - __main__ -     f1 = 83.50502425043705\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}